#!/bin/bash
# Parallel script for Grid Engine on Carbon
#$ -S /bin/bash
#$ -N dnabaseroll-MD
#$ -q parallel
#$ -V
#$ -cwd
#$ -notify
#$ -pe orte 8

# the full-executable name
EXECUTABLE=/share/apps/gromacs/openmpi-intel/gromacs-4.5.4/bin/mdrun_mpi_f
MPI_EXECUTABLE=/usr/mpi/intel/openmpi-1.4.5/bin/mpirun

INPUT_FILE=md

# get the number of hosts back from the queueing system
HOSTS=`gawk {'print $1'} $PE_HOSTFILE | sort | uniq`
NHOSTS=`gawk {'print $1'} $PE_HOSTFILE | sort | uniq | wc -l`

# write out detailed information on the job
MY_HOST=`hostname`
MY_DATE=`date`
echo "Running on $MY_HOST at $MY_DATE"
echo "Running on $MY_HOST at $MY_DATE, jobid: ${JOB_ID%%.*}"  > jobid
echo "================================================================"
echo "PE_HOSTFILE file:"
cat $PE_HOSTFILE
echo This job is allocated on ${NHOSTS} hosts
echo This job is allocated on ${NSLOTS} cpus
echo Job is running on nodes: $HOSTS
echo "================================================================"
echo JOB_NAME=$JOB_NAME
echo JOB_ID=$JOB_ID
echo NSLOTS=$NSLOTS
echo QUEUE=$QUEUE
echo SGE_CWD_PATH=$SGE_CWD_PATH
echo SGE_O_WORKDIR=${SGE_O_WORKDIR}
echo PATH=$PATH
echo SGE_STDIN_PATH=$SGE_STDIN_PATH
echo SGE_STDOUT_PATH=$SGE_STDOUT_PATH
echo SGE_STDERR_PATH=$SGE_STDERR_PATH
echo SGE_O_HOST=$SGE_O_HOST
echo SGE_O_PATH=$SGE_O_PATH
echo NSLOTS=$NSLOTS
echo "================================================================"

# no coredumps
ulimit -S -c 0
ulimit -s unlimited

# set path and library-path for openmpi
export PATH=/usr/mpi/intel/openmpi-1.4.5/bin:${PATH}
export LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:/usr/mpi/intel/openmpi-1.4.5/lib64/
source /share/apps/intel/bin/compilervars.sh intel64

# set the scratch and results-directory
WORKDIR=/state/partition1/$USER/$JOB_NAME-${JOB_ID%%.*}
RESULTS_DIR=${SGE_O_WORKDIR}/../output/$JOB_NAME-${JOB_ID%%.*}

# create the directory in the current directory to store the results
mkdir -p ${RESULTS_DIR}

# do some preparation
# create scratch directories on all hosts
# e.g. VASP and plumed graomacs need the input-data on all nodes
prologue()
{
  # loop over all nodes
  for host in $HOSTS
  do
    # create the scratch directory on the node
    ssh ${host} mkdir -p ${WORKDIR}

    # copy the needed input-files to all the nodes
    scp ${SGE_O_WORKDIR}/* ${host}:${WORKDIR}
  done
}

# detect that the job has been cancelled using 'qdel'
cancelled()
{
  MY_DATE=`date`
  echo "$MY_HOST at $MY_DATE: JOB HAS BEEN CANCELLED!!"
}
trap 'cancelled' SIGUSR2

# run the program
runprogram()
{
  # change to the scratch directory
  cd ${WORKDIR}

  # run program
  time ${MPI_EXECUTABLE} ${EXECUTABLE} -v -deffnm ${INPUT_FILE}  >& o
}

# do cleaning up after the job ended or has been cancelled
epilogue()
{
 MY_DATE=`date`
 echo "Copying files back from $MY_HOST to the headnode at $MY_DATE"

 # copying files from scratch-directory back to home
 if scp -r * 10.1.1.1:${RESULTS_DIR}
 then
   # deleting files from scratch-directories of all HOSTS
   for host in $HOSTS ; do
     echo "Deleting scratch data on host: $host"
     ssh ${host} rm -rf $WORKDIR
   done
 else
   echo "IMPORTANT: Copying the data back from $MYHOST has failed, data is kept on the local nodes"
 fi
}

prologue
runprogram
epilogue

MY_DATE=`date`
echo "Job finished at $MY_DATE"

exit
